# æ€§èƒ½ä¼˜åŒ–æŒ‡å—

> åŸºäºä¸“å®¶è¯„å®¡å»ºè®®å’Œ ClickHouse OLAP æ¶æ„çš„æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ

**ç›®æ ‡**: API å“åº”æ—¶é—´ P50 < 100ms, P99 < 500ms, CDC å»¶è¿Ÿ < 1 ç§’

**æ€§èƒ½æå‡**: 50-1000 å€ï¼ˆvs åŸ PostgreSQL ç›´æ¥æŸ¥è¯¢ï¼‰

---

## ğŸ“Š ä¼˜åŒ–ä¼˜å…ˆçº§

| ä¼˜åŒ–é¡¹ | å½±å“ | ä¼˜å…ˆçº§ | å·¥ä½œé‡ | çŠ¶æ€ |
|--------|------|--------|--------|------|
| ClickHouse OLAP éƒ¨ç½² | â­â­â­â­â­ | ğŸ”´ P0 | 2-3å¤© | å¿…é¡» |
| Debezium CDC + Kafka | â­â­â­â­â­ | ğŸ”´ P0 | 3-4å¤© | å¿…é¡» |
| ClickHouse ç‰©åŒ–è§†å›¾ | â­â­â­â­â­ | ğŸ”´ P0 | 2-3å¤© | å¿…é¡» |
| å¤šå±‚ç¼“å­˜æ¶æ„ | â­â­â­â­ | ğŸ”´ P0 | 1-2å¤© | å¿…é¡» |
| ClickHouse æŸ¥è¯¢ä¼˜åŒ– | â­â­â­ | ğŸŸ¡ P1 | 2å¤© | å»ºè®® |
| æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦ | â­â­â­ | ğŸŸ¡ P1 | 1-2å¤© | å»ºè®® |

---

## ğŸ¯ ä¼˜åŒ–ç­–ç•¥

### 1. ClickHouse OLAP æ¶æ„ï¼ˆå¿…é¡» - P0ï¼‰

#### é—®é¢˜ï¼šOLTP/OLAP æ··ç”¨
```sql
-- âŒ PostgreSQL OLTP æŸ¥è¯¢ï¼ˆæ…¢ï¼š2-5 ç§’ï¼‰
SELECT
  DATE(created_at) as date,
  SUM(amount_total) as revenue,
  COUNT(*) as orders
FROM orders
WHERE merchant_id = 'xxx'
  AND created_at >= NOW() - INTERVAL '30 days'
GROUP BY DATE(created_at);

-- é—®é¢˜:
-- 1. å…¨è¡¨æ‰«æï¼ˆç™¾ä¸‡çº§è®¢å•ï¼‰
-- 2. å®æ—¶èšåˆè®¡ç®—
-- 3. å½±å“ OLTP ä¸šåŠ¡
-- 4. çº¿æ€§æ‰©å±•ç“¶é¢ˆ
```

#### è§£å†³æ–¹æ¡ˆï¼šClickHouse OLAP + CDC

**æ¶æ„æ¦‚è§ˆ**ï¼š
```
PostgreSQL (OLTP) â†’ Debezium CDC â†’ Kafka â†’ ClickHouse (OLAP) â†’ bi-backend
                     (å˜æ›´æ•è·)    (æ¶ˆæ¯é˜Ÿåˆ—)  (ç‰©åŒ–è§†å›¾)      (æŸ¥è¯¢)
                     < 1 ç§’å»¶è¿Ÿ              10-50ms æŸ¥è¯¢
```

**ClickHouse æŸ¥è¯¢**ï¼ˆå¿«ï¼š10-50msï¼‰ï¼š
```sql
-- âœ… ClickHouse ç‰©åŒ–è§†å›¾æŸ¥è¯¢
SELECT date, total_revenue, order_count, avg_order_value
FROM daily_sales_mv
WHERE merchant_id = 'xxx'
  AND date >= today() - 30
ORDER BY date DESC;

-- ä¼˜åŠ¿:
-- 1. åˆ—å¼å­˜å‚¨ï¼ˆåªè¯»éœ€è¦çš„åˆ—ï¼‰
-- 2. é¢„èšåˆæ•°æ®ï¼ˆç‰©åŒ–è§†å›¾è‡ªåŠ¨è®¡ç®—ï¼‰
-- 3. åˆ†åŒºè£å‰ªï¼ˆæœˆåº¦åˆ†åŒºï¼‰
-- 4. é›¶ OLTP å½±å“
-- æ‰§è¡Œæ—¶é—´: 10-50 æ¯«ç§’
```

**æ€§èƒ½æå‡**: 50-1000 å€

è¯¦è§: [ADR-006: ClickHouse + CDC æ¶æ„](./architecture/adr-006-clickhouse-olap.md)

---

### 2. ClickHouse ä¼˜åŒ–ç­–ç•¥ï¼ˆå¿…é¡» - P0ï¼‰

#### 2.1 é€‰æ‹©åˆé€‚çš„è¡¨å¼•æ“

```sql
-- âœ… ReplacingMergeTree: å¤„ç† UPDATE æ“ä½œ
CREATE TABLE orders (
    id UUID,
    merchant_id UUID,
    amount_total Decimal(10, 2),
    created_at DateTime,
    updated_at DateTime
)
ENGINE = ReplacingMergeTree(updated_at)  -- æŒ‰ updated_at å»é‡
PARTITION BY toYYYYMM(created_at)        -- æŒ‰æœˆåˆ†åŒº
ORDER BY (merchant_id, created_at, id);  -- æ’åºé”®ï¼ˆä¸»é”®ï¼‰

-- âœ… SummingMergeTree: è‡ªåŠ¨èšåˆæ±‚å’Œ
CREATE MATERIALIZED VIEW daily_sales_mv
ENGINE = SummingMergeTree()
PARTITION BY toYYYYMM(date)
ORDER BY (merchant_id, date)
AS SELECT
    merchant_id,
    toDate(created_at) as date,
    sum(amount_total) as total_revenue,
    count() as order_count
FROM orders
GROUP BY merchant_id, date;
```

#### 2.2 åˆ†åŒºç­–ç•¥

```sql
-- æŒ‰æœˆåˆ†åŒºï¼ˆæ¨èï¼‰
PARTITION BY toYYYYMM(created_at)

-- ä¼˜åŠ¿:
-- 1. åˆ†åŒºè£å‰ªï¼šæŸ¥è¯¢ 2024-01 åªæ‰«æä¸€ä¸ªåˆ†åŒº
-- 2. æ•°æ®ç®¡ç†ï¼šåˆ é™¤æ—§æ•°æ®åªéœ€ DROP PARTITION
-- 3. æ€§èƒ½æå‡ï¼š10-100xï¼ˆvs å…¨è¡¨æ‰«æï¼‰

-- ç¤ºä¾‹ï¼šåˆ é™¤ 2023 å¹´ 1 æœˆæ•°æ®
ALTER TABLE orders DROP PARTITION '202301';
```

#### 2.3 æ’åºé”®ï¼ˆä¸»é”®ï¼‰ä¼˜åŒ–

```sql
-- âœ… ä¼˜å…ˆçº§åŸåˆ™ï¼šé«˜åŸºæ•°åœ¨å‰ï¼Œä½åŸºæ•°åœ¨å
ORDER BY (merchant_id, created_at, id)
-- merchant_id: é«˜åŸºæ•°ï¼ˆ1000+ å•†å®¶ï¼‰
-- created_at: æ—¶é—´æˆ³ï¼ˆå¤©ç„¶é¡ºåºï¼‰
-- id: UUIDï¼ˆå”¯ä¸€æ ‡è¯†ï¼‰

-- âŒ é”™è¯¯ç¤ºä¾‹ï¼šä½åŸºæ•°å­—æ®µåœ¨å‰
ORDER BY (status, merchant_id, created_at)
-- status åªæœ‰ 5-10 ä¸ªå€¼ï¼Œç´¢å¼•æ•ˆç‡ä½

-- æŸ¥è¯¢ç¤ºä¾‹ï¼ˆåˆ©ç”¨æ’åºé”®ï¼‰:
SELECT * FROM orders
WHERE merchant_id = 'xxx'
  AND created_at >= '2024-01-01'
-- æ‰§è¡Œæ—¶é—´: < 10msï¼ˆä¸»é”®ç´¢å¼•ï¼‰
```

#### 2.4 å‹ç¼©ä¼˜åŒ–

```sql
-- ClickHouse é»˜è®¤ä½¿ç”¨ LZ4 å‹ç¼©
-- å‹ç¼©æ¯”: 10:1ï¼ˆvs åŸå§‹æ•°æ®ï¼‰
-- 1 äº¿è¡Œè®¢å• â‰ˆ 10GB å­˜å‚¨

-- æŸ¥çœ‹å‹ç¼©ç»Ÿè®¡
SELECT
    table,
    formatReadableSize(sum(bytes_on_disk)) as size,
    formatReadableSize(sum(data_uncompressed_bytes)) as uncompressed,
    round(sum(data_uncompressed_bytes) / sum(bytes_on_disk), 2) as ratio
FROM system.parts
WHERE table = 'orders'
GROUP BY table;
```

#### 2.5 æŸ¥è¯¢æ€§èƒ½éªŒè¯

```sql
-- ä½¿ç”¨ EXPLAIN åˆ†ææŸ¥è¯¢
EXPLAIN
SELECT * FROM orders
WHERE merchant_id = 'xxx'
  AND created_at >= today() - 30;

-- é¢„æœŸè¾“å‡º:
-- Expression
--   Filter (merchant_id = 'xxx')
--   ReadFromMergeTree (orders)
--     Prewhere: toYYYYMM(created_at) IN (202401, 202402)  -- åˆ†åŒºè£å‰ª
--     Where: merchant_id = 'xxx'

-- æŸ¥è¯¢ç»Ÿè®¡
SELECT
    query_duration_ms,
    read_rows,
    read_bytes,
    memory_usage
FROM system.query_log
WHERE query LIKE '%daily_sales_mv%'
ORDER BY event_time DESC
LIMIT 10;
```

---

### 3. å¤šå±‚ç¼“å­˜ç­–ç•¥ï¼ˆå¿…é¡» - P0ï¼‰

#### å››å±‚ç¼“å­˜æ¶æ„

```
æŸ¥è¯¢è¯·æ±‚
  â†“
L1: NodeCache å†…å­˜ç¼“å­˜ (1 åˆ†é’Ÿ) â† æçƒ­æ•°æ®
  â†“ miss
L2: Redis ç¼“å­˜ (5 åˆ†é’Ÿ) â† çƒ­æ•°æ®
  â†“ miss
L3: ClickHouse ç‰©åŒ–è§†å›¾ (å®æ—¶) â† æ¸©æ•°æ®ï¼ˆ10-50msï¼‰
  â†“ miss
L4: ClickHouse åŸå§‹è¡¨ (å®æ—¶) â† å†·æ•°æ®ï¼ˆ50-200msï¼‰
```

**æŸ¥è¯¢ä¼˜å…ˆçº§**ï¼š
1. L1/L2: ç¼“å­˜å‘½ä¸­ â†’ ç›´æ¥è¿”å›ï¼ˆ< 10msï¼‰
2. L3: ClickHouse ç‰©åŒ–è§†å›¾ â†’ é¢„èšåˆæ•°æ®ï¼ˆ10-50msï¼‰
3. L4: ClickHouse åŸå§‹è¡¨ â†’ å®æ—¶èšåˆï¼ˆ50-200msï¼‰

#### å®ç°ä»£ç 

```typescript
// src/services/cache.service.ts
import NodeCache from 'node-cache';
import { Redis } from 'ioredis';

export class CacheService {
  private memCache: NodeCache;
  private redis: Redis;

  constructor() {
    this.memCache = new NodeCache({ stdTTL: 60 }); // L1: 1 åˆ†é’Ÿ
    this.redis = new Redis(process.env.REDIS_URL);
  }

  async get<T>(key: string): Promise<T | null> {
    // L1: å†…å­˜ç¼“å­˜
    const memData = this.memCache.get<T>(key);
    if (memData) {
      logger.debug({ cache: 'L1_HIT', key });
      return memData;
    }

    // L2: Redis ç¼“å­˜
    const redisData = await this.redis.get(key);
    if (redisData) {
      const data = JSON.parse(redisData) as T;
      this.memCache.set(key, data); // å†™å…¥ L1
      logger.debug({ cache: 'L2_HIT', key });
      return data;
    }

    logger.debug({ cache: 'MISS', key });
    return null;
  }

  async set<T>(key: string, data: T, ttl: number = 300): Promise<void> {
    // å†™å…¥ L1 (å†…å­˜)
    this.memCache.set(key, data);

    // å†™å…¥ L2 (Redis)
    await this.redis.set(key, JSON.stringify(data), 'EX', ttl);
  }

  async del(key: string): Promise<void> {
    this.memCache.del(key);
    await this.redis.del(key);
  }
}
```

#### é˜²æ­¢ç¼“å­˜å‡»ç©¿

```typescript
// ä½¿ç”¨åˆ†å¸ƒå¼é”é˜²æ­¢å¹¶å‘æŸ¥è¯¢
async getSalesDataWithLock(merchantId: string, days: number) {
  const cacheKey = `sales:${merchantId}:${days}`;
  const lockKey = `lock:${cacheKey}`;

  // å°è¯•è·å–ç¼“å­˜
  let data = await this.cacheService.get(cacheKey);
  if (data) return data;

  // è·å–åˆ†å¸ƒå¼é”
  const lock = await this.redis.set(lockKey, '1', 'EX', 10, 'NX');

  if (lock) {
    try {
      // è·å–é”æˆåŠŸï¼ŒæŸ¥è¯¢æ•°æ®åº“
      data = await this.querySalesFromDB(merchantId, days);

      // å†™å…¥ç¼“å­˜
      await this.cacheService.set(cacheKey, data, 300);

      return data;
    } finally {
      // é‡Šæ”¾é”
      await this.redis.del(lockKey);
    }
  } else {
    // è·å–é”å¤±è´¥ï¼Œç­‰å¾…åé‡è¯•
    await sleep(50);
    return this.getSalesDataWithLock(merchantId, days);
  }
}
```

---

### 4. SQL æŸ¥è¯¢ä¼˜åŒ–ï¼ˆå»ºè®® - P1ï¼‰

#### æŸ¥è¯¢ä¼˜åŒ–æ¸…å•

##### âœ… é¿å… SELECT *
```sql
-- âŒ ä¸å¥½
SELECT * FROM orders WHERE merchant_id = 'xxx';

-- âœ… å¥½
SELECT id, order_number, amount_total, created_at
FROM orders WHERE merchant_id = 'xxx';
```

##### âœ… ä½¿ç”¨ LIMIT
```sql
-- âŒ ä¸å¥½
SELECT * FROM orders WHERE merchant_id = 'xxx';

-- âœ… å¥½
SELECT id, amount_total FROM orders
WHERE merchant_id = 'xxx'
ORDER BY created_at DESC
LIMIT 100;
```

##### âœ… é¿å…å­æŸ¥è¯¢
```sql
-- âŒ ä¸å¥½
SELECT * FROM orders
WHERE merchant_id IN (
  SELECT id FROM merchants WHERE is_active = true
);

-- âœ… å¥½
SELECT o.* FROM orders o
JOIN merchants m ON o.merchant_id = m.id
WHERE m.is_active = true;
```

##### âœ… ä½¿ç”¨ EXISTS æ›¿ä»£ IN
```sql
-- âŒ ä¸å¥½
SELECT * FROM products
WHERE id IN (SELECT product_id FROM order_items);

-- âœ… å¥½
SELECT * FROM products p
WHERE EXISTS (
  SELECT 1 FROM order_items oi WHERE oi.product_id = p.id
);
```

##### âœ… æ‰¹é‡æ“ä½œ
```typescript
// âŒ ä¸å¥½ï¼šN+1 æŸ¥è¯¢
for (const orderId of orderIds) {
  const order = await prisma.order.findUnique({ where: { id: orderId } });
}

// âœ… å¥½ï¼šæ‰¹é‡æŸ¥è¯¢
const orders = await prisma.order.findMany({
  where: { id: { in: orderIds } }
});
```

---

### 5. è¿æ¥æ± ä¼˜åŒ–ï¼ˆå»ºè®® - P1ï¼‰

#### Prisma è¿æ¥æ± é…ç½®

```typescript
// prisma/schema.prisma
datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

// .env
DATABASE_URL="postgresql://user:pass@host:5432/db?
  connection_limit=10&
  pool_timeout=10&
  connect_timeout=10"
```

#### è¿æ¥æ± æœ€ä½³å®è·µ

```typescript
// src/db/prisma.ts
import { PrismaClient } from '@prisma/client';

const globalForPrisma = global as unknown as { prisma: PrismaClient };

export const prisma =
  globalForPrisma.prisma ||
  new PrismaClient({
    log: process.env.NODE_ENV === 'development'
      ? ['query', 'error', 'warn']
      : ['error'],
    datasources: {
      db: {
        url: process.env.DATABASE_URL,
      },
    },
  });

if (process.env.NODE_ENV !== 'production') {
  globalForPrisma.prisma = prisma;
}

// ä¼˜é›…å…³é—­
process.on('SIGTERM', async () => {
  await prisma.$disconnect();
  process.exit(0);
});
```

**è¿æ¥æ± å¤§å°å»ºè®®**:
- **å¼€å‘ç¯å¢ƒ**: 5-10 è¿æ¥
- **ç”Ÿäº§ç¯å¢ƒ**: 20-50 è¿æ¥ï¼ˆæ ¹æ®å¹¶å‘é‡è°ƒæ•´ï¼‰
- **è®¡ç®—å…¬å¼**: `è¿æ¥æ•° = (æ ¸å¿ƒæ•° Ã— 2) + ç£ç›˜æ•°`

---

## ğŸ“ˆ æ€§èƒ½ç›‘æ§

### å…³é”®æŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡ | è­¦å‘Šé˜ˆå€¼ | ä¸¥é‡é˜ˆå€¼ |
|------|------|---------|---------|
| **API å±‚** |
| API å“åº”æ—¶é—´ (P50) | < 100ms | > 200ms | > 500ms |
| API å“åº”æ—¶é—´ (P99) | < 500ms | > 1s | > 2s |
| é”™è¯¯ç‡ | < 0.1% | > 1% | > 5% |
| **ç¼“å­˜å±‚** |
| ç¼“å­˜å‘½ä¸­ç‡ (L1+L2) | > 70% | < 50% | < 30% |
| **ClickHouse å±‚** |
| ClickHouse æŸ¥è¯¢æ—¶é—´ | < 50ms | > 100ms | > 200ms |
| ClickHouse æ…¢æŸ¥è¯¢ | 0 | > 5/å°æ—¶ | > 20/å°æ—¶ |
| **CDC å±‚** |
| CDC æ•°æ®å»¶è¿Ÿ | < 1s | > 3s | > 10s |
| Kafka æ¶ˆè´¹å»¶è¿Ÿ | < 500ms | > 2s | > 5s |
| Kafka æ¶ˆæ¯å †ç§¯ | 0 | > 1000 | > 10000 |

### ç›‘æ§å®ç°

```typescript
// src/middleware/metrics.ts
import { FastifyRequest, FastifyReply } from 'fastify';

export async function metricsMiddleware(
  request: FastifyRequest,
  reply: FastifyReply
) {
  const start = Date.now();

  reply.addHook('onSend', async () => {
    const duration = Date.now() - start;

    // è®°å½•æŒ‡æ ‡
    logger.info({
      type: 'api_metrics',
      method: request.method,
      url: request.url,
      statusCode: reply.statusCode,
      duration,
      merchantId: request.user?.merchantId,
      dataSource: request.dataSource,  // 'CLICKHOUSE_MV' | 'CLICKHOUSE_RAW'
      cacheHit: request.cacheHit,      // 'L1' | 'L2' | null
      clickhouseDuration: request.clickhouseDuration,
    });

    // æ…¢æŸ¥è¯¢å‘Šè­¦ï¼ˆClickHouse æ¶æ„ä¸‹é˜ˆå€¼æ›´ä½ï¼‰
    if (duration > 500) {
      logger.warn({
        type: 'slow_api_request',
        duration,
        url: request.url,
        dataSource: request.dataSource,
      });
    }

    // ClickHouse æ…¢æŸ¥è¯¢å‘Šè­¦
    if (request.clickhouseDuration > 100) {
      logger.warn({
        type: 'slow_clickhouse_query',
        duration: request.clickhouseDuration,
        query: request.clickhouseQuery,
      });
    }
  });
}

// src/services/monitoring.service.ts
export class MonitoringService {
  // CDC å»¶è¿Ÿç›‘æ§
  async monitorCdcLatency() {
    const pgLatest = await prisma.order.findFirst({
      orderBy: { createdAt: 'desc' },
      select: { createdAt: true }
    });

    const chResult = await clickhouse.query({
      query: `SELECT max(created_at) as latest FROM orders`,
      format: 'JSONEachRow',
    });
    const chLatest = await chResult.json();

    const latency = pgLatest.createdAt - chLatest[0].latest;

    logger.info({
      type: 'cdc_latency',
      latency_ms: latency,
      pg_latest: pgLatest.createdAt,
      ch_latest: chLatest[0].latest,
    });

    if (latency > 3000) {
      logger.warn({ type: 'cdc_latency_alert', latency_ms: latency });
    }
  }
}
```

---

## ğŸ§ª æ€§èƒ½æµ‹è¯•

### åŸºå‡†æµ‹è¯•ï¼ˆClickHouse æ¶æ„ï¼‰

```bash
# ä½¿ç”¨ Apache Bench è¿›è¡ŒåŸºå‡†æµ‹è¯•
ab -n 1000 -c 50 \
   -H "Authorization: Bearer <token>" \
   https://bi-api.optima.chat/api/v1/sales?days=7

# é¢„æœŸç»“æœï¼ˆClickHouse æ¶æ„ï¼‰:
# Requests per second: > 100 req/sï¼ˆvs åŸ 50 req/sï¼‰
# Time per request (mean): < 100msï¼ˆvs åŸ 200msï¼‰
# Time per request (50th percentile): < 100ms
# Time per request (99th percentile): < 500msï¼ˆvs åŸ 2000msï¼‰
```

### å‹åŠ›æµ‹è¯•

```bash
# ä½¿ç”¨ k6 è¿›è¡Œå‹åŠ›æµ‹è¯•
k6 run - <<EOF
import http from 'k6/http';
import { check } from 'k6';

export let options = {
  stages: [
    { duration: '2m', target: 100 },  // å‡è‡³ 100 å¹¶å‘
    { duration: '5m', target: 100 },  // ä¿æŒ 100 å¹¶å‘
    { duration: '2m', target: 200 },  // å‡è‡³ 200 å¹¶å‘
    { duration: '5m', target: 200 },  // ä¿æŒ 200 å¹¶å‘
    { duration: '2m', target: 500 },  // å‡è‡³ 500 å¹¶å‘ï¼ˆæé™æµ‹è¯•ï¼‰
    { duration: '3m', target: 500 },  // ä¿æŒ 500 å¹¶å‘
    { duration: '2m', target: 0 },    // é™è‡³ 0
  ],
};

export default function() {
  let response = http.get('https://bi-api.optima.chat/api/v1/sales?days=7', {
    headers: { 'Authorization': 'Bearer <token>' },
  });

  check(response, {
    'status is 200': (r) => r.status === 200,
    'response time < 500ms': (r) => r.timings.duration < 500,  // ClickHouse ç›®æ ‡
    'response time < 1s': (r) => r.timings.duration < 1000,
  });
}
EOF

# é¢„æœŸç»“æœ:
# âœ… 100 å¹¶å‘: P95 < 200ms
# âœ… 200 å¹¶å‘: P95 < 300ms
# âœ… 500 å¹¶å‘: P95 < 500ms
```

### CDC å»¶è¿Ÿæµ‹è¯•

```bash
# æµ‹è¯• PostgreSQL â†’ ClickHouse CDC å»¶è¿Ÿ
# 1. åœ¨ PostgreSQL æ’å…¥è®¢å•
psql -c "INSERT INTO orders (...) VALUES (...);" -c "SELECT now();"

# 2. ç­‰å¾… 1 ç§’
sleep 1

# 3. åœ¨ ClickHouse æŸ¥è¯¢è®¢å•
clickhouse-client --query "SELECT * FROM orders WHERE id = 'xxx';" --query "SELECT now();"

# é¢„æœŸç»“æœ:
# âœ… CDC å»¶è¿Ÿ < 1 ç§’
# âœ… è®¢å•æ•°æ®å·²åŒæ­¥åˆ° ClickHouse
```

### æ•°æ®è§„æ¨¡æµ‹è¯•ï¼ˆClickHouseï¼‰

```sql
-- ClickHouse æ•°æ®è§„æ¨¡æµ‹è¯•
-- å¯¼å…¥ 1000 ä¸‡å†å²è®¢å•åˆ° ClickHouse

-- 1. æµ‹è¯• ClickHouse ç‰©åŒ–è§†å›¾æŸ¥è¯¢ï¼ˆ90 å¤©é”€å”®ï¼‰
SELECT * FROM daily_sales_mv
WHERE merchant_id = 'merchant_test'
  AND date >= today() - 90
ORDER BY date DESC;

-- é¢„æœŸ: < 50msï¼ˆvs PostgreSQL 2-5sï¼‰
-- æ€§èƒ½æå‡: 40-100x

-- 2. æµ‹è¯• ClickHouse åŸå§‹è¡¨æŸ¥è¯¢ï¼ˆ7 å¤©é”€å”®ï¼‰
SELECT
    toDate(created_at) as date,
    sum(amount_total) as revenue,
    count() as orders
FROM orders
WHERE merchant_id = 'merchant_test'
  AND created_at >= today() - 7
  AND status IN ('paid', 'delivered')
GROUP BY date
ORDER BY date DESC;

-- é¢„æœŸ: < 100msï¼ˆvs PostgreSQL 2-5sï¼‰
-- æ€§èƒ½æå‡: 20-50x

-- 3. æµ‹è¯•å•†å“ Top 10 æŸ¥è¯¢
SELECT
    product_id,
    sum(quantity) as total_quantity,
    sum(amount) as total_revenue
FROM order_items
WHERE merchant_id = 'merchant_test'
  AND created_at >= today() - 30
GROUP BY product_id
ORDER BY total_revenue DESC
LIMIT 10;

-- é¢„æœŸ: < 50msï¼ˆvs PostgreSQL 3-8sï¼‰
-- æ€§èƒ½æå‡: 60-160x
```

---

## âœ… æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•

### Phase 1: ClickHouse + CDC éƒ¨ç½²ï¼ˆå¿…é¡»å®Œæˆ - P0ï¼‰

- [ ] **ClickHouse éƒ¨ç½²**
  - [ ] ClickHouse å•èŠ‚ç‚¹éƒ¨ç½²ï¼ˆDocker Composeï¼‰
  - [ ] åˆ›å»º orders è¡¨ï¼ˆReplacingMergeTreeï¼‰
  - [ ] åˆ›å»º order_items è¡¨ï¼ˆReplacingMergeTreeï¼‰
  - [ ] åˆ›å»º products è¡¨ï¼ˆReplacingMergeTreeï¼‰
  - [ ] é…ç½®åˆ†åŒºç­–ç•¥ï¼ˆæŒ‰æœˆ PARTITION BY toYYYYMMï¼‰

- [ ] **ClickHouse ç‰©åŒ–è§†å›¾**
  - [ ] daily_sales_mvï¼ˆSummingMergeTreeï¼‰
  - [ ] hourly_sales_mvï¼ˆSummingMergeTreeï¼‰
  - [ ] product_stats_mvï¼ˆSummingMergeTreeï¼‰
  - [ ] customer_stats_mvï¼ˆSummingMergeTreeï¼‰
  - [ ] merchant_overview_mvï¼ˆSummingMergeTreeï¼‰

- [ ] **Debezium CDC + Kafka**
  - [ ] Kafka + Zookeeper éƒ¨ç½²
  - [ ] Debezium Connect éƒ¨ç½²
  - [ ] PostgreSQL Logical Replication é…ç½®
  - [ ] åˆ›å»º Publicationï¼ˆdbz_publicationï¼‰
  - [ ] é…ç½® Debezium Connector
  - [ ] éªŒè¯ CDC æµç¨‹ï¼ˆ< 1 ç§’å»¶è¿Ÿï¼‰

- [ ] **ClickHouse Kafka Engine**
  - [ ] åˆ›å»º orders_kafka è¡¨
  - [ ] åˆ›å»º orders_consumer ç‰©åŒ–è§†å›¾
  - [ ] åˆ›å»ºå…¶ä»– Kafka æ¶ˆè´¹è€…è¡¨
  - [ ] éªŒè¯æ¶ˆæ¯æ¶ˆè´¹

- [ ] **bi-backend é›†æˆ ClickHouse**
  - [ ] å®‰è£… @clickhouse/client
  - [ ] åˆ›å»º ClickHouse æœåŠ¡å±‚
  - [ ] é‡æ„æŸ¥è¯¢æœåŠ¡ï¼ˆæŸ¥è¯¢ç‰©åŒ–è§†å›¾ï¼‰
  - [ ] é›†æˆå¤šå±‚ç¼“å­˜

- [ ] **å¤šå±‚ç¼“å­˜æ¶æ„**
  - [ ] L1 å†…å­˜ç¼“å­˜ï¼ˆNodeCacheï¼Œ1 åˆ†é’Ÿï¼‰
  - [ ] L2 Redis ç¼“å­˜ï¼ˆ5 åˆ†é’Ÿï¼‰
  - [ ] L3 ClickHouse ç‰©åŒ–è§†å›¾
  - [ ] L4 ClickHouse åŸå§‹è¡¨
  - [ ] åˆ†å¸ƒå¼é”ï¼ˆé˜²å‡»ç©¿ï¼‰

- [ ] **æ€§èƒ½æµ‹è¯•**
  - [ ] åŸºå‡†æµ‹è¯•ï¼ˆP50 < 100ms, P99 < 500msï¼‰
  - [ ] å‹åŠ›æµ‹è¯•ï¼ˆ500 å¹¶å‘ï¼‰
  - [ ] CDC å»¶è¿Ÿæµ‹è¯•ï¼ˆ< 1 ç§’ï¼‰
  - [ ] æ•°æ®è§„æ¨¡æµ‹è¯•ï¼ˆåƒä¸‡çº§è®¢å•ï¼‰
  - [ ] æ€§èƒ½æŠ¥å‘Šï¼ˆ50-1000x æå‡éªŒè¯ï¼‰

### Phase 2: ç›‘æ§å’Œä¼˜åŒ–ï¼ˆå»ºè®®å®Œæˆ - P1ï¼‰

- [ ] **ClickHouse æŸ¥è¯¢ä¼˜åŒ–**
  - [ ] ä½¿ç”¨ EXPLAIN åˆ†ææ‰€æœ‰æŸ¥è¯¢
  - [ ] ä¼˜åŒ–æ’åºé”®ï¼ˆORDER BYï¼‰
  - [ ] ä¼˜åŒ–åˆ†åŒºç­–ç•¥
  - [ ] æŸ¥çœ‹å‹ç¼©ç»Ÿè®¡

- [ ] **ç›‘æ§å’Œå‘Šè­¦**
  - [ ] API å“åº”æ—¶é—´ç›‘æ§ï¼ˆ< 100msï¼‰
  - [ ] ClickHouse æŸ¥è¯¢æ—¶é—´ç›‘æ§ï¼ˆ< 50msï¼‰
  - [ ] CDC å»¶è¿Ÿç›‘æ§ï¼ˆ< 1 ç§’ï¼‰
  - [ ] Kafka æ¶ˆè´¹å»¶è¿Ÿç›‘æ§ï¼ˆ< 500msï¼‰
  - [ ] ç¼“å­˜å‘½ä¸­ç‡ç›‘æ§ï¼ˆ> 70%ï¼‰
  - [ ] æ…¢æŸ¥è¯¢å‘Šè­¦
  - [ ] é”™è¯¯ç‡ç›‘æ§

- [ ] **è¿æ¥æ± ä¼˜åŒ–**
  - [ ] è¿æ¥æ± å¤§å°è°ƒæ•´
  - [ ] è¿æ¥è¶…æ—¶é…ç½®
  - [ ] è¿æ¥æ³„æ¼æ£€æµ‹

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [PostgreSQL Performance Optimization](https://wiki.postgresql.org/wiki/Performance_Optimization)
- [Redis Best Practices](https://redis.io/docs/manual/patterns/)
- [Prisma Performance Best Practices](https://www.prisma.io/docs/guides/performance-and-optimization)
- [ä¸“å®¶è¯„å®¡æŠ¥å‘Š](./expert-review.md)

---

**ç»´æŠ¤è€…**: Optima BI Team
**æœ€åæ›´æ–°**: 2025-01-21
